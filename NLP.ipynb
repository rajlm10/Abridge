{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMhW63oPoqoT7C15tQ/8ci3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Il8Kuv4FTEcW"},"source":["import nltk\n","import re\n","from nltk.tokenize import word_tokenize\n","import numpy as np\n","from collections import Counter\n","nltk.data.path.append('.')\n","from scipy import linalg\n","from collections import defaultdict\n","def sigmoid(z):\n","    # sigmoid function\n","    return 1.0/(1.0+np.exp(-z))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DG1qu3rYQFKD"},"source":["def get_idx(words, word2Ind):\n","    idx = []\n","    for word in words:\n","        idx = idx + [word2Ind[word]]\n","    return idx\n","\n","\n","def pack_idx_with_frequency(context_words, word2Ind):\n","    freq_dict = defaultdict(int)\n","    for word in context_words:\n","        freq_dict[word] += 1\n","    idxs = get_idx(context_words, word2Ind)\n","    packed = []\n","    for i in range(len(idxs)):\n","        idx = idxs[i]\n","        freq = freq_dict[context_words[i]]\n","        packed.append((idx, freq))\n","    return packed\n","\n","\n","#Given corpus, break it up into arrays of context words and center word, send the context word array to \n","#the function above and each words frequency in the corpus, then divide this vector by num of context words in the array\n","# Thus x would look like [0.25, 0.25, 0.  , 0.5 , 0.  ] for vocab of size 5\n","#At the same time y is a one hot vector with the center word's position turned on\n","#For example [0,0,0,1,0]\n","\n","def get_vectors(data, word2Ind, V, C):\n","    i = C\n","    while True:\n","        y = np.zeros(V)\n","        x = np.zeros(V)\n","        center_word = data[i]\n","        y[word2Ind[center_word]] = 1\n","        context_words = data[(i - C):i] + data[(i+1):(i+C+1)]\n","        num_ctx_words = len(context_words)\n","        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n","            x[idx] = freq/num_ctx_words\n","        yield x, y\n","        i += 1\n","        if i >= len(data):\n","            print('i is being set to 0')\n","            i = 0\n","\n","\n","\n","\n","\n","def get_batches(data, word2Ind, V, C, batch_size):\n","    batch_x = []\n","    batch_y = []\n","    for x, y in get_vectors(data, word2Ind, V, C):\n","        while len(batch_x) < batch_size:\n","            batch_x.append(x)\n","            batch_y.append(y)\n","        else:\n","            yield np.array(batch_x).T, np.array(batch_y).T\n","            batch = []\n","  \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQC5RnOQQKy5"},"source":["#Returns word_dict: a dictionary with the weighted probabilities of each word\n","        #word2Ind: returns dictionary mapping the word to its index\n","        #Ind2Word: returns dictionary mapping the index to its word\n","def get_dict(data):\n","    words = sorted(list(set(data)))\n","    n = len(words)\n","    idx = 0\n","    word2Ind = {}\n","    Ind2word = {}\n","    for k in words:\n","        word2Ind[k] = idx\n","        Ind2word[idx] = k\n","        idx += 1\n","    return word2Ind, Ind2word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrUkVaPsQkKW","executionInfo":{"status":"ok","timestamp":1604142863550,"user_tz":-330,"elapsed":1819,"user":{"displayName":"RAJ BHARAT SANGANI 18BCE1197","photoUrl":"","userId":"12680828564208249961"}},"outputId":"cb60e2c5-dccc-4f8d-e6cf-0d323260c198","colab":{"base_uri":"https://localhost:8080/"}},"source":["data=\"In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, 'With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. Thatâ€™s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.' The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"\n","data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n","data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n","data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n","print(\"Number of tokens:\", len(data),'\\n', data[:15]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of tokens: 328 \n"," ['in', 'an', 'attempt', 'to', 'build', 'an', 'workforce', '.', 'microsoft', 'announced', 'intelligent', 'cloud', 'hub', 'which', 'has']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RTWwa7iNRjEp","executionInfo":{"status":"ok","timestamp":1604142863551,"user_tz":-330,"elapsed":1815,"user":{"displayName":"RAJ BHARAT SANGANI 18BCE1197","photoUrl":"","userId":"12680828564208249961"}},"outputId":"d2c3c25f-5e28-4126-dd04-763ee26e89e3","colab":{"base_uri":"https://localhost:8080/"}},"source":["word2Ind, Ind2word = get_dict(data)\n","V = len(word2Ind)\n","print(\"Size of vocabulary: \", V)\n","# example of word to index mapping\n","print(\"Index of the word 'workforce' :  \",word2Ind['workforce'] )\n","print(\"Word which has index 5:  \",Ind2word[5] )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of vocabulary:  155\n","Index of the word 'workforce' :   152\n","Word which has index 5:   aims\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NyHrJxkaR86Y"},"source":["def initialize_model(N,V, random_seed=1):\n","    '''\n","    Inputs: \n","        N:  dimension of hidden vector \n","        V:  dimension of vocabulary\n","        random_seed: random seed for consistent results in the unit tests\n","     Outputs: \n","        W1, W2, b1, b2: initialized weights and biases\n","    '''\n","    \n","    np.random.seed(random_seed)\n","    \n","   \n","    # W1 has shape (N,V)\n","    W1 = np.random.rand(N,V)\n","    # W2 has shape (V,N)\n","    W2 = np.random.rand(V,N)\n","    # b1 has shape (N,1)\n","    b1 = np.random.rand(N,1)\n","    # b2 has shape (V,1)\n","    b2 = np.random.rand(V,1)\n","\n","    return W1, W2, b1, b2\n","\n","\n","#Gives probs for each possible word of V summing to 1\n","def softmax(z):\n","    '''\n"," prediction (estimate of y)\n","    '''\n","    \n","    yhat = np.exp(z)/np.sum(np.exp(z),axis=0)\n","    \n","    \n","    return yhat\n","\n","\n","def forward_prop(x, W1, W2, b1, b2):\n","    '''\n","    Inputs: \n","        x:  average one hot vector for the context \n","        W1, W2, b1, b2:  matrices and biases to be learned\n","     Outputs: \n","        z:  output score vector\n","    '''\n","    \n","    \n","    # Calculate h\n","    h = np.dot(W1,x)+b1\n","    \n","    # Apply the relu on h \n","    h = np.maximum(0,h)\n","    \n","    \n","    # Calculate z\n","    z = np.dot(W2,h)+b2\n","    \n","\n","    return z, h\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bg3JPK5_SXIw"},"source":["def compute_cost(y, yhat, batch_size):\n","    # cost function \n","    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n","    cost = - 1/batch_size * np.sum(logprobs)\n","    cost = np.squeeze(cost)\n","    return cost\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDLBq9NhThbD"},"source":["def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n","\n","    l1 = np.dot(W2.T,yhat-y)\n","    # Apply relu to l1\n","    l1 = np.maximum(0,l1)\n","    # Compute the gradient of W1\n","    grad_W1 = (1/batch_size)*np.dot(l1,x.T) \n","    # Compute the gradient of W2\n","    grad_W2 = (1/batch_size)*np.dot(yhat-y,h.T)\n","    # Compute the gradient of b1\n","    grad_b1 = np.sum((1/batch_size)*np.dot(l1,x.T),axis=1,keepdims=True)\n","    # Compute the gradient of b2\n","    grad_b2 = np.sum((1/batch_size)*np.dot(yhat-y,h.T),axis=1,keepdims=True)\n","   \\\n","    \n","    return grad_W1, grad_W2, grad_b1, grad_b2\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvCdZG-5TyqK","executionInfo":{"status":"ok","timestamp":1604142991848,"user_tz":-330,"elapsed":4760,"user":{"displayName":"RAJ BHARAT SANGANI 18BCE1197","photoUrl":"","userId":"12680828564208249961"}},"outputId":"bb09ce3e-5bcb-47d2-99a1-9c40d8ad1e4c","colab":{"base_uri":"https://localhost:8080/"}},"source":["def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n","    \n"," \n","    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n","    batch_size = 128\n","    iters = 0\n","    C = 2\n","    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n","       \n","        # Get z and h\n","        z, h = forward_prop(x,W1,W2,b1,b2)\n","        # Get yhat\n","        yhat = softmax(z)\n","        # Get cost\n","        cost = compute_cost(y,yhat,batch_size)\n","        if ( (iters+1) % 10 == 0):\n","            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n","        # Get gradients\n","        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n","        \n","        # Update weights and biases\n","        W1 -= alpha*grad_W1 \n","        W2 -= alpha*grad_W2\n","        b1 -= alpha*grad_b1\n","        b2 -= alpha*grad_b2\n","        \n","       \n","        \n","        iters += 1 \n","        if iters == num_iters: \n","            break\n","        if iters % 100 == 0:\n","            alpha *= 0.66\n","            \n","    return W1, W2, b1, b2\n","W1, W2, b1, b2 = gradient_descent(data, word2Ind, 50, len(word2Ind), 500)\n","\n","#Retreive embeddings of size V X N\n","embs = (W1.T + W2)/2.0\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["iters: 10 cost: 0.070543\n","iters: 20 cost: 0.031579\n","iters: 30 cost: 0.020415\n","iters: 40 cost: 0.015098\n","iters: 50 cost: 0.011985\n","iters: 60 cost: 0.009938\n","iters: 70 cost: 0.008490\n","iters: 80 cost: 0.007412\n","iters: 90 cost: 0.006576\n","iters: 100 cost: 0.005911\n","iters: 110 cost: 0.005524\n","iters: 120 cost: 0.005200\n","iters: 130 cost: 0.004912\n","iters: 140 cost: 0.004654\n","iters: 150 cost: 0.004422\n","iters: 160 cost: 0.004213\n","iters: 170 cost: 0.004022\n","iters: 180 cost: 0.003847\n","iters: 190 cost: 0.003688\n","iters: 200 cost: 0.003541\n","iters: 210 cost: 0.003445\n","iters: 220 cost: 0.003359\n","iters: 230 cost: 0.003278\n","iters: 240 cost: 0.003200\n","iters: 250 cost: 0.003126\n","iters: 260 cost: 0.003055\n","iters: 270 cost: 0.002987\n","iters: 280 cost: 0.002922\n","iters: 290 cost: 0.002860\n","iters: 300 cost: 0.002801\n","iters: 310 cost: 0.002761\n","iters: 320 cost: 0.002724\n","i is being set to 0\n","iters: 330 cost: 0.002688\n","iters: 340 cost: 0.002653\n","iters: 350 cost: 0.002619\n","iters: 360 cost: 0.002586\n","iters: 370 cost: 0.002554\n","iters: 380 cost: 0.002522\n","iters: 390 cost: 0.002492\n","iters: 400 cost: 0.002462\n","iters: 410 cost: 0.002441\n","iters: 420 cost: 0.002422\n","iters: 430 cost: 0.002403\n","iters: 440 cost: 0.002385\n","iters: 450 cost: 0.002366\n","iters: 460 cost: 0.002349\n","iters: 470 cost: 0.002331\n","iters: 480 cost: 0.002313\n","iters: 490 cost: 0.002296\n","iters: 500 cost: 0.002279\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z21rDkzMULyV","executionInfo":{"status":"ok","timestamp":1604143012264,"user_tz":-330,"elapsed":1912,"user":{"displayName":"RAJ BHARAT SANGANI 18BCE1197","photoUrl":"","userId":"12680828564208249961"}},"outputId":"11ba1b8c-dfa1-48b6-ea83-903958533bd5","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Showing an example of similarity in our corpus between words education and learning\n","#As you can see 0.84 is pretty accurate and shows they are very similar\n","from nltk.cluster.util import cosine_distance\n","\n","1-cosine_distance(embs[word2Ind['education']],embs[word2Ind['learning']])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8405726777600854"]},"metadata":{"tags":[]},"execution_count":16}]}]}